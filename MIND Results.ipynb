{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f365740-9598-4788-994a-c0abce7b1c07",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "import ast\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a845e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_float64(s):\n",
    "    # Extract all float numbers inside the np.float64(...) constructs\n",
    "    float_strings = re.findall(r'np\\.float64\\(([^)]+)\\)', s)\n",
    "    # Convert to Python float\n",
    "    return [float(num) for num in float_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c606a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = pd.read_csv('results/mind_results_k@10.csv', index_col=0)\n",
    "\n",
    "results['topic_calibrations'] = results['topic_calibrations'].apply(lambda x: ast.literal_eval(x))\n",
    "results['subtopic_calibrations'] = results['subtopic_calibrations'].apply(lambda x: ast.literal_eval(x))\n",
    "results['complexity_calibrations'] = results['complexity_calibrations'].apply(lambda x: ast.literal_eval(x))\n",
    "results['fragmentations'] = results['fragmentations'].apply(lambda x: parse_list_float64(x))\n",
    "results['activations'] = results['activations'].apply(lambda x: ast.literal_eval(x))\n",
    "results['representations'] = results['representations'].apply(lambda x: ast.literal_eval(x))\n",
    "results['alternative_voices'] = results['alternative_voices'].apply(lambda x: ast.literal_eval(x))\n",
    "results['tf_idf_ild_values'] = results['tf_idf_ild_values'].apply(lambda x: ast.literal_eval(x))\n",
    "results['sentbert_ild_values'] = results['sentbert_ild_values'].apply(lambda x: ast.literal_eval(x))\n",
    "results['gini_values'] = results['gini_values'].apply(lambda x: ast.literal_eval(x))\n",
    "results['ndcg_values'] = results['ndcg_values'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d6d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "metrics = ['topic_calibrations', 'subtopic_calibrations', 'complexity_calibrations', 'activations', 'tf_idf_ild_values', 'sentbert_ild_values', 'gini_values', 'ndcg_values']\n",
    "metric_titles = {\n",
    "    'topic_calibrations': 'Topic Calibration Score',\n",
    "    'subtopic_calibrations': 'Subtopic Calibration Score', \n",
    "    'complexity_calibrations': 'Complexity Calibration Score',\n",
    "    'activations': 'Activation Score',\n",
    "    'tf_idf_ild_values': 'TF-IDF ILD Score',\n",
    "    'sentbert_ild_values': 'SentBERT ILD Score',\n",
    "    'gini_values': 'Gini Score',\n",
    "    'ndcg_values': 'NDCG Score'\n",
    "}\n",
    "\n",
    "for metric in metrics:\n",
    "    df_naml   = pd.DataFrame({'value': results[metric]['naml'], 'type': 'NAML'})\n",
    "    df_nrms   = pd.DataFrame({'value': results[metric]['nrms'], 'type': 'NRMS'})\n",
    "    df_lstur  = pd.DataFrame({'value': results[metric]['lstur'], 'type': 'LSTUR'})\n",
    "    df_random = pd.DataFrame({'value': results[metric]['random'], 'type': 'Random'})\n",
    "    df_original_random = pd.DataFrame({'value': results[metric]['incorrect_random'], 'type': 'Original Random'})\n",
    "    df_all    = pd.concat([df_naml, df_nrms, df_lstur, df_random, df_original_random])\n",
    "\n",
    "    sample = (\n",
    "        df_all\n",
    "        .groupby('type', group_keys=False)\n",
    "        .apply(lambda d: (\n",
    "            d.sample(5, random_state=1)\n",
    "             .assign(sample_id=lambda df: np.arange(len(df)))\n",
    "        ))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    density = alt.Chart(df_all).transform_density(\n",
    "        'value',\n",
    "        as_=['value','density'],\n",
    "        groupby=['type'],\n",
    "        extent=[df_all.value.min(), df_all.value.max()],\n",
    "        bandwidth=(df_all.value.max() - df_all.value.min())/20\n",
    "    ).mark_area(opacity=0.3).encode(\n",
    "        x=alt.X('value:Q', title=metric_titles[metric]),\n",
    "        y=alt.Y('density:Q', title='Probability Density', stack=None),\n",
    "        color=alt.Color(\n",
    "            'type:N',\n",
    "            title='Method',\n",
    "            scale=alt.Scale(domain=['NAML', 'NRMS', 'LSTUR', 'Random', 'Original Random']),\n",
    "            legend=alt.Legend(\n",
    "                titleFontSize=16,\n",
    "                labelFontSize=14,\n",
    "                symbolSize=150,\n",
    "                padding=10,\n",
    "                orient='right',\n",
    "                direction='vertical',\n",
    "                legendX=20,\n",
    "                legendY=20,\n",
    "                fillColor='white',\n",
    "                symbolOpacity=1,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    sample_points = alt.Chart(sample).mark_point(\n",
    "        filled=True,\n",
    "        size=100,\n",
    "        opacity=1\n",
    "    ).encode(\n",
    "        x='value:Q',\n",
    "        y=alt.Y('type:N', \n",
    "                title=None, \n",
    "                axis=None, \n",
    "                sort=['NAML','NRMS','LSTUR','Random','Original Random'],\n",
    "                scale=alt.Scale(padding=40)\n",
    "               ),\n",
    "        shape=alt.Shape(\n",
    "            'sample_id:O',\n",
    "            scale=alt.Scale(domain=list(range(5)), range=['circle','square','triangle','diamond','cross']),\n",
    "            legend=None\n",
    "        ),\n",
    "        color=alt.Color('type:N', legend=None)\n",
    "    )\n",
    "\n",
    "    chart = (density + sample_points).properties(\n",
    "        width=600, height=300\n",
    "    ).configure_title(\n",
    "        fontSize=20,\n",
    "    ).configure_axis(\n",
    "        titleFontSize=16,\n",
    "        labelFontSize=14\n",
    "    )\n",
    "\n",
    "    # Save with higher resolution\n",
    "    # chart.save(f'results/mind_{metric}_distributions.png', scale_factor=3.0)\n",
    "    display(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results = results.applymap(lambda x: np.mean(x) if isinstance(x, list) else x)\n",
    "display(aggregated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52638cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['topic_calibrations', 'subtopic_calibrations', 'complexity_calibrations', 'activations', 'tf_idf_ild_values', 'sentbert_ild_values', 'gini_values', 'ndcg_values']\n",
    "\n",
    "# Perform significance testing between two methods. In this case, we compare LSTUR and random.\n",
    "for metric in metrics:\n",
    "    lstur = results[metric]['pop']\n",
    "    random = results[metric]['random']\n",
    "    t, p = ttest_ind(lstur, random)\n",
    "    print(f'{metric} t: {t}, p: {p}')\n",
    "    if p < 0.01:\n",
    "        print(f'{metric} is significant')\n",
    "    else: \n",
    "        print(f'{metric} is not significant')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f545f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code plots the mean of all metrics for each method as the number of samples increases.\n",
    "\n",
    "metrics = ['topic_calibrations', 'subtopic_calibrations', 'complexity_calibrations', 'activations', \n",
    "          'tf_idf_ild_values', 'sentbert_ild_values', 'gini_values', 'ndcg_values']\n",
    "\n",
    "for metric_to_plot in metrics:\n",
    "    x = np.arange(1, 201)\n",
    "    y1 = [] \n",
    "    y2 = []\n",
    "    y3 = []\n",
    "    y4 = []\n",
    "    \n",
    "    for seed in range(10):\n",
    "        y1_seed = []\n",
    "        y2_seed = []\n",
    "        y3_seed = []\n",
    "        y4_seed = []\n",
    "\n",
    "        start = seed * 200\n",
    "\n",
    "        for n in x:\n",
    "            end = start + n\n",
    "            y1_seed.append(np.mean(results[metric_to_plot]['random'][start:end]))\n",
    "            y2_seed.append(np.mean(results[metric_to_plot]['nrms'][start:end]))\n",
    "            y3_seed.append(np.mean(results[metric_to_plot]['naml'][start:end]))\n",
    "            y4_seed.append(np.mean(results[metric_to_plot]['lstur'][start:end]))\n",
    "            \n",
    "        y1.append(y1_seed)\n",
    "        y2.append(y2_seed)\n",
    "        y3.append(y3_seed)\n",
    "        y4.append(y4_seed)\n",
    "\n",
    "    y1 = np.array(y1)\n",
    "    y2 = np.array(y2)\n",
    "    y3 = np.array(y3)\n",
    "    y4 = np.array(y4)\n",
    "\n",
    "    plot_1 = np.mean(y1, axis=0)\n",
    "    plot_2 = np.mean(y2, axis=0)\n",
    "    plot_3 = np.mean(y3, axis=0)\n",
    "    plot_4 = np.mean(y4, axis=0)\n",
    "\n",
    "    # Calculate standard errors\n",
    "    std_1 = 2 * np.std(y1, axis=0) / np.sqrt(y1.shape[0])\n",
    "    std_2 = 2 * np.std(y2, axis=0) / np.sqrt(y2.shape[0])\n",
    "    std_3 = 2 * np.std(y3, axis=0) / np.sqrt(y3.shape[0])\n",
    "    std_4 = 2 * np.std(y4, axis=0) / np.sqrt(y4.shape[0])\n",
    "\n",
    "    x_values = np.arange(len(plot_1))\n",
    "\n",
    "    # Create individual DataFrames for each plot with error bars\n",
    "    df1 = pd.DataFrame({\n",
    "        'x': x_values, \n",
    "        'mean': plot_1, \n",
    "        'std': std_1,\n",
    "        'metric': 'Random'\n",
    "    })\n",
    "    df2 = pd.DataFrame({\n",
    "        'x': x_values, \n",
    "        'mean': plot_2, \n",
    "        'std': std_2,\n",
    "        'metric': 'NRMS'\n",
    "    })\n",
    "    df3 = pd.DataFrame({\n",
    "        'x': x_values, \n",
    "        'mean': plot_3, \n",
    "        'std': std_3,\n",
    "        'metric': 'NAML'\n",
    "    })\n",
    "    df4 = pd.DataFrame({\n",
    "        'x': x_values, \n",
    "        'mean': plot_4, \n",
    "        'std': std_4,\n",
    "        'metric': 'LSTUR'\n",
    "    })\n",
    "\n",
    "    # Combine all DataFrames into one\n",
    "    df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "    df['upper'] = df['mean'] + df['std']\n",
    "    df['lower'] = df['mean'] - df['std']\n",
    "\n",
    "    # Create the base line chart\n",
    "    base = alt.Chart(df).mark_line().encode(\n",
    "        x=alt.X('x', title='Number of samples', axis=alt.Axis(labelFontSize=14, titleFontSize=16)),\n",
    "        y=alt.Y('mean', title=metric_to_plot.replace('_', ' ').title(), \n",
    "                scale=alt.Scale(domain=[(df['lower'].min() - 0.1*(df['upper'].max()-df['lower'].min())),\n",
    "                                         (df['upper'].max() + 0.1*(df['upper'].max()-df['lower'].min()))]),\n",
    "                axis=alt.Axis(labelFontSize=14, titleFontSize=16)),\n",
    "        color=alt.Color('metric:N', scale=alt.Scale(scheme='category10'), \n",
    "        legend=alt.Legend(\n",
    "                title='Method',\n",
    "                orient='right',\n",
    "                labelFontSize=14,\n",
    "                titleFontSize=16,\n",
    "                padding=10,\n",
    "                symbolOpacity=1,\n",
    "                symbolFillColor='transparent',\n",
    "                direction='vertical',\n",
    "            )),\n",
    "    ).properties(\n",
    "        width=500,\n",
    "        height=200,\n",
    "    )\n",
    "\n",
    "    # Create error bands\n",
    "    error_bands = alt.Chart(df).mark_area(opacity=0.2).encode(\n",
    "        x='x',\n",
    "        y='lower:Q',\n",
    "        y2='upper:Q',\n",
    "        color='metric:N'\n",
    "    )\n",
    "\n",
    "    # Combine and save\n",
    "    final_chart = (base + error_bands).properties(\n",
    "        width=500,\n",
    "        height=200,\n",
    "    )\n",
    "\n",
    "    # Save at high resolution with metric-specific filename\n",
    "    filename = f'results/mind_converging_{metric_to_plot}.png'\n",
    "    final_chart.save(filename, scale_factor=3.0)\n",
    "    display(final_chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc714ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ndo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
